{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2EVAeKxA-jY",
    "outputId": "b2220ce6-24cc-44a3-d6a7-552423c51ada",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "import psycopg2\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INDEX_TO_WORD_FILE = \"word_to_index.pickle\"\n",
    "STEPS_BEGINNING_FILE = \"steps_beginning.pickle\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n67xrXyWA-jf",
    "outputId": "5bc6b119-4998-4a6e-e072-9475f707f987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps recipes count: 10303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Cook bacon in a skillet over medium heat until crisp.|Remove bacon from pan, reserving drippings, and crumble.|Add the shrimp to pan, and cook 2 minutes, turning once.|Combine bacon, 2 cups fennel, grape tomatoes, red onion, and baby spinach in a bowl.|Combine the remaining ingredients in a small bowl, stirring with a whisk.|Add the shrimp and balsamic mixture to spinach mixture; toss well.',\n",
       " 'In a large bowl, whisk together the milk and eggs. Stir in the sour cream, spinach, asparagus, Parmesan, and salt.|Add the flour and mix until well combined.|In a large skillet, over medium heat, heat 1/3 of the oil. Make 4 pancakes (about 1/4 cup of batter per pancake) and cook until golden brown and slightly puffed, 2 to 3 minutes per side. Working in batches, make a total of 12 pancakes, adding oil as necessary.']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"157.230.24.228\",\n",
    "    database=\"cookix_db\",\n",
    "    user=\"cookix_user_db\",\n",
    "    password=\"f9d6UVP6gxEqueopMCiKdpjC0A5Pi5Ww\",\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT steps FROM recipes_recipe;\")\n",
    "data = cursor.fetchall()\n",
    "data = [steps[0] for steps in data]\n",
    "\n",
    "clean_data = [steps for steps in data if steps.strip() != \"\"]\n",
    "print(f\"steps recipes count: {len(clean_data)}\" )\n",
    "clean_data = clean_data\n",
    "clean_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ko9wrfFbA-jg",
    "outputId": "e6b9a027-58bc-4fb0-bae9-057d69203607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 1559139\n",
      "Unique Words: 11759\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess_text(sentences: list):\n",
    "    \n",
    "    processed_sen = []\n",
    "    all_words = []\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for sen in sentences:\n",
    "        \n",
    "        if not (\"Ingredients\" in sen or \"Save Recipe\" in sen or \"Print Recipe\" in sen):\n",
    "            \n",
    "            # replace the pipe \"|\" that we used for steps separation, by space\n",
    "            sentence = re.sub(r\"(\\||:|!|\\?|=|\\*|\\{|\\}|\\'|\\-\\-|Â°|\\$|&|''|``|\\(|\\)|\\[|\\]|#|%)\", ' ', sen.lower())\n",
    "            sentence = re.sub(r\"[0-9]+f|[0-9]+\\-(inch)?\", ' ', sentence)\n",
    "            sentence = re.sub(r';', ',', sentence)\n",
    "            sentence = re.sub(r\"\\-?>\", ',', sentence)\n",
    "            \n",
    "            # replace numbers by [number] (valid for 5, 5555, 55-5555 ....)\n",
    "            sentence = re.sub(r\"\\s+[0-9]+((\\-|/)[0-9]*)*\\s+\", ' __NUMBER__ ', sentence)\n",
    "            sentence = re.sub(r\"[0-9]+/?[0-9]*\", ' __NUMBER__ ', sentence)\n",
    "            sentence = re.sub(r'(type_27_data|init_step_by_step_images)', ' ', sentence)\n",
    "            # remove brackets\n",
    "            sentence = re.sub(r'\\(.+\\)', ' ', sentence)\n",
    "            # remove meaningless comments\n",
    "            sentence = re.sub(r'Serves.*', ' ', sentence)\n",
    "            # remove meaningless comments\n",
    "            sentence = re.sub(r'reprinted.*', ' ', sentence)\n",
    "            \n",
    "            sentence = re.sub(r'\\.', ' . ', sentence)\n",
    "            # Removing multiple spaces\n",
    "            sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "            sen_tokens = word_tokenize(sentence)\n",
    "            #sen_tokens = [w for w in sen_tokens if not w in stop_words]\n",
    "            sentence = [lemmatizer.lemmatize(w) for w in sen_tokens]\n",
    "            [all_words.append(w) for w in sentence]\n",
    "            processed_sen.append(sentence)    \n",
    "\n",
    "    return processed_sen, all_words\n",
    "\n",
    "processed_sen, all_words = preprocess_text(clean_data)\n",
    "\n",
    "n_words = len(all_words)\n",
    "unique_words = list(set(all_words))\n",
    "n_unique_words = len(unique_words)\n",
    "\n",
    "print('Total Words: %d' % n_words)\n",
    "print('Unique Words: %d' % n_unique_words)\n",
    "# Total Words: 1079113\n",
    "# Unique Words: 11631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEIpd2ZqA-ji",
    "outputId": "51721a8f-6a41-41c7-aac3-7a7225ade499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1406391, 15, 1)\n",
      "y shape: (1406391, 1)\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word = {}, {}\n",
    "for i, w in enumerate(unique_words): \n",
    "    index_to_word[i] = w\n",
    "    word_to_index[w] = i\n",
    "\n",
    "input_sequence = []\n",
    "output_words = []\n",
    "steps_beginning_idx = []\n",
    "input_seq_length = 15\n",
    "\n",
    "for sen in processed_sen:\n",
    "    for i in range(len(sen) - input_seq_length):\n",
    "        in_seq = sen[i:i + input_seq_length]\n",
    "        out_seq = sen[i + input_seq_length]\n",
    "        in_seq_index = [word_to_index[word] for word in in_seq]\n",
    "        input_sequence.append(in_seq_index)\n",
    "        output_words.append(word_to_index[out_seq])\n",
    "        if i == 0 and in_seq_index not in steps_beginning_idx: \n",
    "            steps_beginning_idx.append(in_seq_index)\n",
    "            \n",
    "\n",
    "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
    "# X = X / float(vocab_size)\n",
    "# y = to_categorical(output_words)\n",
    "y = np.array(output_words.copy())\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# save our variables\n",
    "\n",
    "with open(INDEX_TO_WORD_FILE, 'wb') as file:\n",
    "    pickle.dump(index_to_word, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(STEPS_BEGINNING_FILE, 'wb') as file:\n",
    "    pickle.dump(steps_beginning_idx, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xf0sHuYHA-jk",
    "outputId": "252c50b6-c3ce-4a76-a34a-8cb603868a22",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         2351800   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 11759)             764335    \n",
      "=================================================================\n",
      "Total params: 3,491,591\n",
      "Trainable params: 3,491,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 1/100\n",
      "5494/5494 [==============================] - 274s 48ms/step - loss: 4.1877 - accuracy: 0.2393\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.18767, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 2/100\n",
      "5494/5494 [==============================] - 260s 47ms/step - loss: 3.3832 - accuracy: 0.3491\n",
      "\n",
      "Epoch 00002: loss improved from 4.18767 to 3.38316, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 3/100\n",
      "5494/5494 [==============================] - 263s 48ms/step - loss: 3.1746 - accuracy: 0.3766\n",
      "\n",
      "Epoch 00003: loss improved from 3.38316 to 3.17460, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 4/100\n",
      "5494/5494 [==============================] - 264s 48ms/step - loss: 3.0594 - accuracy: 0.3920\n",
      "\n",
      "Epoch 00004: loss improved from 3.17460 to 3.05937, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 5/100\n",
      "5494/5494 [==============================] - 256s 47ms/step - loss: 2.9849 - accuracy: 0.4017\n",
      "\n",
      "Epoch 00005: loss improved from 3.05937 to 2.98485, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 6/100\n",
      "5494/5494 [==============================] - 257s 47ms/step - loss: 2.9311 - accuracy: 0.4085\n",
      "\n",
      "Epoch 00006: loss improved from 2.98485 to 2.93106, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 7/100\n",
      "5494/5494 [==============================] - 258s 47ms/step - loss: 2.8883 - accuracy: 0.4142\n",
      "\n",
      "Epoch 00007: loss improved from 2.93106 to 2.88834, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 8/100\n",
      "5494/5494 [==============================] - 262s 48ms/step - loss: 2.8565 - accuracy: 0.4189\n",
      "\n",
      "Epoch 00008: loss improved from 2.88834 to 2.85652, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 9/100\n",
      "5494/5494 [==============================] - 259s 47ms/step - loss: 2.8277 - accuracy: 0.4231\n",
      "\n",
      "Epoch 00009: loss improved from 2.85652 to 2.82773, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 10/100\n",
      "5494/5494 [==============================] - 258s 47ms/step - loss: 2.8057 - accuracy: 0.4262\n",
      "\n",
      "Epoch 00010: loss improved from 2.82773 to 2.80567, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 11/100\n",
      "5494/5494 [==============================] - 254s 46ms/step - loss: 2.7840 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00011: loss improved from 2.80567 to 2.78402, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 12/100\n",
      "5494/5494 [==============================] - 267s 49ms/step - loss: 2.7664 - accuracy: 0.4321\n",
      "\n",
      "Epoch 00012: loss improved from 2.78402 to 2.76643, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 13/100\n",
      "5494/5494 [==============================] - 264s 48ms/step - loss: 2.7506 - accuracy: 0.4343\n",
      "\n",
      "Epoch 00013: loss improved from 2.76643 to 2.75057, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 14/100\n",
      "5494/5494 [==============================] - 264s 48ms/step - loss: 2.7367 - accuracy: 0.4363\n",
      "\n",
      "Epoch 00014: loss improved from 2.75057 to 2.73673, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 15/100\n",
      "5494/5494 [==============================] - 262s 48ms/step - loss: 2.7236 - accuracy: 0.4385\n",
      "\n",
      "Epoch 00015: loss improved from 2.73673 to 2.72358, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 16/100\n",
      "5494/5494 [==============================] - 261s 48ms/step - loss: 2.7111 - accuracy: 0.4404\n",
      "\n",
      "Epoch 00016: loss improved from 2.72358 to 2.71106, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 17/100\n",
      "5494/5494 [==============================] - 263s 48ms/step - loss: 2.7005 - accuracy: 0.4420\n",
      "\n",
      "Epoch 00017: loss improved from 2.71106 to 2.70055, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 18/100\n",
      "5494/5494 [==============================] - 263s 48ms/step - loss: 2.6896 - accuracy: 0.4435\n",
      "\n",
      "Epoch 00018: loss improved from 2.70055 to 2.68959, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 19/100\n",
      "5494/5494 [==============================] - 262s 48ms/step - loss: 2.6804 - accuracy: 0.4450\n",
      "\n",
      "Epoch 00019: loss improved from 2.68959 to 2.68045, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 20/100\n",
      "5494/5494 [==============================] - 259s 47ms/step - loss: 2.6732 - accuracy: 0.4460\n",
      "\n",
      "Epoch 00020: loss improved from 2.68045 to 2.67320, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 21/100\n",
      "5494/5494 [==============================] - 258s 47ms/step - loss: 2.6644 - accuracy: 0.4471\n",
      "\n",
      "Epoch 00021: loss improved from 2.67320 to 2.66443, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 22/100\n",
      "5494/5494 [==============================] - 255s 46ms/step - loss: 2.6574 - accuracy: 0.4485\n",
      "\n",
      "Epoch 00022: loss improved from 2.66443 to 2.65741, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 23/100\n",
      "5494/5494 [==============================] - 255s 46ms/step - loss: 2.6506 - accuracy: 0.4497\n",
      "\n",
      "Epoch 00023: loss improved from 2.65741 to 2.65060, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 24/100\n",
      "5494/5494 [==============================] - 256s 47ms/step - loss: 2.6430 - accuracy: 0.4506\n",
      "\n",
      "Epoch 00024: loss improved from 2.65060 to 2.64295, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 25/100\n",
      "5494/5494 [==============================] - 255s 46ms/step - loss: 2.6361 - accuracy: 0.4513\n",
      "\n",
      "Epoch 00025: loss improved from 2.64295 to 2.63611, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 26/100\n",
      "5494/5494 [==============================] - 255s 46ms/step - loss: 2.6312 - accuracy: 0.4527\n",
      "\n",
      "Epoch 00026: loss improved from 2.63611 to 2.63115, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 27/100\n",
      "5494/5494 [==============================] - 255s 46ms/step - loss: 2.6252 - accuracy: 0.4532\n",
      "\n",
      "Epoch 00027: loss improved from 2.63115 to 2.62518, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 28/100\n",
      "5494/5494 [==============================] - 254s 46ms/step - loss: 2.6207 - accuracy: 0.4539\n",
      "\n",
      "Epoch 00028: loss improved from 2.62518 to 2.62065, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 29/100\n",
      "5494/5494 [==============================] - 253s 46ms/step - loss: 2.6153 - accuracy: 0.4549\n",
      "\n",
      "Epoch 00029: loss improved from 2.62065 to 2.61529, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 30/100\n",
      "5494/5494 [==============================] - 252s 46ms/step - loss: 2.6106 - accuracy: 0.4557\n",
      "\n",
      "Epoch 00030: loss improved from 2.61529 to 2.61058, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 31/100\n",
      "5494/5494 [==============================] - 252s 46ms/step - loss: 2.6073 - accuracy: 0.4565\n",
      "\n",
      "Epoch 00031: loss improved from 2.61058 to 2.60726, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 32/100\n",
      "5494/5494 [==============================] - 250s 45ms/step - loss: 2.6021 - accuracy: 0.4568\n",
      "\n",
      "Epoch 00032: loss improved from 2.60726 to 2.60212, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 33/100\n",
      "5494/5494 [==============================] - 251s 46ms/step - loss: 2.5985 - accuracy: 0.4573\n",
      "\n",
      "Epoch 00033: loss improved from 2.60212 to 2.59851, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 34/100\n",
      "5494/5494 [==============================] - 250s 45ms/step - loss: 2.5942 - accuracy: 0.4585\n",
      "\n",
      "Epoch 00034: loss improved from 2.59851 to 2.59415, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 35/100\n",
      "5494/5494 [==============================] - 250s 46ms/step - loss: 2.5919 - accuracy: 0.4586\n",
      "\n",
      "Epoch 00035: loss improved from 2.59415 to 2.59193, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 36/100\n",
      "5494/5494 [==============================] - 250s 45ms/step - loss: 2.5876 - accuracy: 0.4594\n",
      "\n",
      "Epoch 00036: loss improved from 2.59193 to 2.58765, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 37/100\n",
      "5494/5494 [==============================] - 249s 45ms/step - loss: 2.5873 - accuracy: 0.4594\n",
      "\n",
      "Epoch 00037: loss improved from 2.58765 to 2.58729, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 38/100\n",
      "5494/5494 [==============================] - 249s 45ms/step - loss: 2.5835 - accuracy: 0.4602\n",
      "\n",
      "Epoch 00038: loss improved from 2.58729 to 2.58348, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 39/100\n",
      "5494/5494 [==============================] - 252s 46ms/step - loss: 2.5806 - accuracy: 0.4602\n",
      "\n",
      "Epoch 00039: loss improved from 2.58348 to 2.58063, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 40/100\n",
      "5494/5494 [==============================] - 252s 46ms/step - loss: 2.5779 - accuracy: 0.4609\n",
      "\n",
      "Epoch 00040: loss improved from 2.58063 to 2.57792, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 41/100\n",
      "5494/5494 [==============================] - 253s 46ms/step - loss: 2.5744 - accuracy: 0.4617\n",
      "\n",
      "Epoch 00041: loss improved from 2.57792 to 2.57435, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 42/100\n",
      "5494/5494 [==============================] - 253s 46ms/step - loss: 2.5713 - accuracy: 0.4619\n",
      "\n",
      "Epoch 00042: loss improved from 2.57435 to 2.57126, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 43/100\n",
      "5494/5494 [==============================] - 250s 46ms/step - loss: 2.5702 - accuracy: 0.4622\n",
      "\n",
      "Epoch 00043: loss improved from 2.57126 to 2.57025, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 44/100\n",
      "5494/5494 [==============================] - 249s 45ms/step - loss: 2.5662 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00044: loss improved from 2.57025 to 2.56619, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 45/100\n",
      "5494/5494 [==============================] - 248s 45ms/step - loss: 2.5654 - accuracy: 0.4627\n",
      "\n",
      "Epoch 00045: loss improved from 2.56619 to 2.56543, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 46/100\n",
      "5494/5494 [==============================] - 249s 45ms/step - loss: 2.5642 - accuracy: 0.4633\n",
      "\n",
      "Epoch 00046: loss improved from 2.56543 to 2.56417, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 47/100\n",
      "5494/5494 [==============================] - 253s 46ms/step - loss: 2.5604 - accuracy: 0.4639\n",
      "\n",
      "Epoch 00047: loss improved from 2.56417 to 2.56040, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 48/100\n",
      "5494/5494 [==============================] - 250s 45ms/step - loss: 2.5592 - accuracy: 0.4640\n",
      "\n",
      "Epoch 00048: loss improved from 2.56040 to 2.55919, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 49/100\n",
      "5494/5494 [==============================] - 250s 45ms/step - loss: 2.5582 - accuracy: 0.4640\n",
      "\n",
      "Epoch 00049: loss improved from 2.55919 to 2.55818, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 50/100\n",
      "5494/5494 [==============================] - 250s 46ms/step - loss: 2.5569 - accuracy: 0.4643\n",
      "\n",
      "Epoch 00050: loss improved from 2.55818 to 2.55693, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 51/100\n",
      "5494/5494 [==============================] - 255s 46ms/step - loss: 2.5561 - accuracy: 0.4647\n",
      "\n",
      "Epoch 00051: loss improved from 2.55693 to 2.55609, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 52/100\n",
      "5494/5494 [==============================] - 252s 46ms/step - loss: 2.5549 - accuracy: 0.4648\n",
      "\n",
      "Epoch 00052: loss improved from 2.55609 to 2.55487, saving model to gen_weights_202106151707_seq15_1406391sen_11759vocab_ep100.h5\n",
      "Epoch 53/100\n",
      " 145/5494 [..............................] - ETA: 4:04 - loss: 2.4953 - accuracy: 0.4752"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e12321cd95c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT_GEN_WEIGHTS_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# model.load_weights(TEXT_GEN_WEIGHTS_NAME+\".hdf5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_unique_words, 200))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(n_unique_words, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "current_time = datetime.today().strftime(\"%Y%m%d%H%M\")\n",
    "TEXT_GEN_WEIGHTS_NAME = f\"gen_weights_{current_time}_seq{input_seq_length}_{X.shape[0]}sen_{n_unique_words}vocab_ep{EPOCHS}.h5\"\n",
    "\n",
    "print(TEXT_GEN_WEIGHTS_NAME)\n",
    "checkpoint = ModelCheckpoint(TEXT_GEN_WEIGHTS_NAME, monitor='loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(X, y, batch_size=256, epochs=EPOCHS, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "# model.load_weights(TEXT_GEN_WEIGHTS_NAME+\".hdf5\")\n",
    "# model.compile(loss=LOSS, optimizer=OPTIMIZER)\n",
    "# os.remove(TEXT_GEN_WEIGHTS_NAME+\".hdf5\")\n",
    "model.save(TEXT_GEN_WEIGHTS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "flE5J_yfA-jl",
    "outputId": "d8ac4c7c-f099-45a0-d6a3-ad97baec279e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9dn38c+VhLCvIexLEHDBDTTgUleoLdW6tVrR1q1W72pR2972qbZ9bOvT5W57321xqVatVquCS8VSb6pVxH0j7CJCQggQQAhLIBBCSHI9f8wJDCExM5AzM5l836/XvJjzO8tc5+jMlev8zvkdc3dERERilZHsAEREpHVR4hARkbgocYiISFyUOEREJC5KHCIiEpesZAeQCL179/a8vLxkhyEi0qrMnTt3k7vnNmxvE4kjLy+PgoKCZIchItKqmNmqxtp1qkpEROKixCEiInFR4hARkbgocYiISFyUOEREJC5KHCIiEhclDhERiYsSh4hIGtq4vYpfzVzK5h27W3zbShwiImnogTeK+cvbK9mxu6bFt63EISKSZjZWVPHkB6u4eMxAhuZ0bvHtK3GIiKSZP79RTE2dM/nsEaFsX4lDRCSNlFXs5skPVnHh6AHk9W75agOUOERE0sqDb66guqaOm8ePDO0zlDhERNLEph27+dv7q7hw9ECGhVRtgBKHiEjaeOjNYqpr6pg8Ppy+jXpKHCIiaWDzjt08/t4qLjh+AMNzu4T6WUocIiJp4KG3VlJVU8vkEPs26ilxiIi0clt2VvP4eyWcf9wARvQJt9qAkBOHmU00s2VmVmRmtzcy/wwzm2dmNWZ2SVT70KB9gZktMbNvR8070cwWB9u828wszH0QEUl1D71VzK49tdwyIdy+jXqhJQ4zywTuA74EjAIuN7NRDRZbDVwDPNWgfT1wiruPBk4CbjezAcG8+4HrgZHBa2IoOyAi0gps3VnN4++WcN6x/RnRp2tCPjPMimMcUOTuxe5eDUwDLoxewN1L3H0RUNegvdrd60fmal8fp5n1B7q5+/vu7sDjwEUh7oOISEp7+O1iKvfUcsuE8Ps26oWZOAYCa6KmS4O2mJjZYDNbFGzjN+6+Lli/NJZtmtkNZlZgZgVlZWVxBy8ikurKK6t57N1VnHtMfw7vm5hqA1K4c9zd17j7ccAI4Goz6xvn+g+6e7675+fm5oYTpIhIEtWPfpvIagPCTRxrgcFR04OCtrgElcZHwOnB+oMOdZsiIq1deWU1f32nhHOP7ccR/RJXbUC4iWMOMNLMhplZNjAJmBHLimY2yMw6Bu97AqcBy9x9PbDdzE4Orqa6CvhHOOGLiKSuR95eSUUSqg0IMXG4ew0wGXgZWAo84+5LzOwuM7sAwMzGmlkpcCnwZzNbEqx+FPCBmS0E3gD+290XB/NuAh4GioAVwL/C2gcRkVS0rXIPj75TwsSj+3Fkv24J//ysMDfu7jOBmQ3a7ox6P4f9Tz3Vt78CHNfENguAY1o2UhGR1uORd5JXbUAKd46LiMiBtu3awyPvrOQLo/oyakDiqw1Q4hARaVX++k4JFVXJqzZAiUNEpNXYXrWHv7xdzOeP6ssxA7snLQ4lDhGRVuKxd0rYXlXDdz+fvGoDlDhERFqFiqo9PPz2Sj5/VJ+kVhugxCEi0io89m4J23bt4dYJhyc7FCUOEZFUt2N3DQ+/vZLxR/bh2EHJrTZAiUNEJOU99m4J5ZV7uDWJV1JFU+IQEUlhO3fX8PBbxZx1RC7HD+6R7HAAJQ4RkZT2+Hur2JpC1QYocYiIpKydu2t46K1izjw8lzFDeiY7nL2UOEREUtTf3l/Flp3V3Jrk+zYaUuIQEUlBldU1PPRmMaeP7M0JKVRtgBKHiEhKeuL9VWzeWZ30u8Qbo8QhIpJidlXX8uCbxZw2ojcnDu2V7HAOoMQhIpJinvxgFZt2pF7fRj0lDhGRFLKrupYH3ijm1OE5jM1LvWoDlDhERFLKUx+uZtOO3Sl130ZDShwiIimiak8tD7yxglMOy+Gkw3KSHU6TlDhERFLEUx+spqxid8r2bdRT4hARSQH11cZJw3pxcgpXG6DEISKSEqZ9uJqNraDaACUOEZGkq9pTy/1vrGBcXi9OSfFqA0JOHGY20cyWmVmRmd3eyPwzzGyemdWY2SVR7aPN7D0zW2Jmi8zssqh5fzWzlWa2IHiNDnMfRETC9kzBGjZsj1QbZpbscJqVFdaGzSwTuA84BygF5pjZDHf/OGqx1cA1wG0NVq8ErnL3QjMbAMw1s5fdvTyY/wN3fy6s2EVEEmV3TS33v76CsXk9OXV46lcbEG7FMQ4ocvdid68GpgEXRi/g7iXuvgioa9C+3N0Lg/frgI1AboixiogkxTMFpazfVsWtEw5vFdUGhJs4BgJroqZLg7a4mNk4IBtYEdX8y+AU1h/MrH0T691gZgVmVlBWVhbvx4qIhG53TS1/ml3EiUN78rkRraPagBTvHDez/sDfgGvdvb4quQM4EhgL9AJ+2Ni67v6gu+e7e35urooVEUk9z+6tNlpH30a9MBPHWmBw1PSgoC0mZtYN+F/gx+7+fn27u6/3iN3Ao0ROiYmItCrVNXXc//oKxgzpwekjeyc7nLiEmTjmACPNbJiZZQOTgBmxrBgsPx14vGEneFCFYJH0fBHwUYtGLSKSAM/NLWVt+a5WV21AiInD3WuAycDLwFLgGXdfYmZ3mdkFAGY21sxKgUuBP5vZkmD1rwFnANc0ctntk2a2GFgM9AZ+EdY+iIiEobqmjvtmF3H84B6ceXjrO5Ue2uW4AO4+E5jZoO3OqPdziJzCarjeE8ATTWxzfAuHKSKSUM/Pi1Qbv7j4mFZXbUCKd46LiKSbPbV13Du7iOMHdeesVlhtgBKHiEhCPT+vlNKtu1rNXeKNUeIQEUmQ+mrjuEHdOfuIPskO56ApcYiIJMj0+WtZs2UXt4xvvdUGKHGIiCRETW3kSqpjBnZjwlGtt9oAJQ4RkYR4YcE6Vm2ubPXVBihxiIiErqa2jntfK2RU/26cM6pvssM5ZEocIiIhm7FwHSWbK1v1lVTRlDhEREJUU1vHPa8VcVT/bnwhDaoNUOIQEQnVPxetY+Wmndw6YURaVBugxCEiEpraOuee14o4sl9XvjCqX7LDaTFKHCIiIXlx0TqKy3Zyy4SRZGSkR7UBShwiIqGorXPunlXIEX27MvHo9Kk2QIlDRCQU/7t4PSvKdnLzhBFpVW2AEoeISIurrXPumVXIyD5dOPeY/skOp8UpcYiItLCZi9dTuHFH2vVt1FPiEBFpQXV1zj2vFTKiTxfOPTb9qg1Q4hARaVH/+uhTlm/Ywc3jR5CZhtUGKHGIiLSYuuBKquG5nfnycQOSHU5olDhERFrIy0s+ZdmGCm4ePzJtqw1Q4hARaRF1dc6UWYUc1rsz5x+fvtUGKHGIiLSIf3+8gU8+reDmCenbt1FPiUNE5BDVVxvDenfm/DTu26gXauIws4lmtszMiszs9kbmn2Fm88ysxswuiWofbWbvmdkSM1tkZpdFzRtmZh8E23zazLLD3AcRkea8snQDS9dvZ/LZI8jKTP+/x0PbQzPLBO4DvgSMAi43s1ENFlsNXAM81aC9ErjK3Y8GJgJ/NLMewbzfAH9w9xHAVuC6cPZARKR57pErqfJyOnHh6PSvNiDcimMcUOTuxe5eDUwDLoxewN1L3H0RUNegfbm7Fwbv1wEbgVyLDGY/HnguWPQx4KIQ90FE5DO9unQjS9Zt5zttpNqAcBPHQGBN1HRp0BYXMxsHZAMrgByg3N1rmtummd1gZgVmVlBWVhbvx4qINMvdmTJrOUN6deLiMXH/vLVaKZ0ezaw/8DfgWneva275aO7+oLvnu3t+bm5uOAGKSJv22icb+Wht2+nbqBfTnprZ82Z2npnFc2TWAoOjpgcFbTExs27A/wI/dvf3g+bNQA8zyzqYbYqItJRItVHI4F4dufiEtlNtQOwVx5+AK4BCM/svMzsihnXmACODq6CygUnAjFg+LFh+OvC4u9f3Z+DuDswG6q/Auhr4R4z7ICLSYmYv28ii0m1MPnsE7dpQtQExJg53f9Xdvw6cAJQAr5rZu2Z2rZm1a2KdGmAy8DKwFHjG3ZeY2V1mdgGAmY01s1LgUuDPZrYkWP1rwBnANWa2IHiNDub9EPi+mRUR6fP4y0Hst4jIQXN3prxayKCeHfnKCYOSHU7CZTW/SISZ5QDfAK4E5gNPAqcR+av/rMbWcfeZwMwGbXdGvZ9D5HRTw/WeAJ5oYpvFRK7YEhFJiteXl7GwdBu//sqxba7agBgTh5lNB44g0lF9vruvD2Y9bWYFYQUnIpJq6quNgT068tU2WG1A7BXH3e4+u7EZ7p7fgvGIiKS0Nws3sWBNOb+8+Biys9petQGxd46PirpzGzPraWY3hRSTiEhKilQbyxnQvQOXnji4+RXSVKyJ43p3L6+fcPetwPXhhCQikpreLtrEvNXl3HT2iDZbbUDsiSMzGO4D2DsOlQYXFJE2o75vo3/3Dlya3zb7NurFmjheItIRPsHMJgBTgzYRkTbhnaLNFKzayk1nDad9Vmayw0mqWDvHfwj8B3BjMP0K8HAoEYmIpJj6Man6devA18a23b6NejEljmCcqPuDl4hIm/Leis3MKdnKzy84us1XGxD7fRwjgV8Tea5Gh/p2dz8spLhERFLGH2cV0rdbey5TtQHE3sfxKJFqowY4G3icJu7sFhFJJ++t2MyHK7fw7TOH06Gdqg2IPXF0dPdZgLn7Knf/GXBeeGGJiKSGKbOWk9u1PZePG5LsUFJGrJ3ju4Mh1QvNbDKRocy7hBeWiEjyvV+8mfeLt3Dnl0ep2ogSa8VxK9AJuAU4kchgh1eHFZSISCqY8mohuV3bc8VJqjaiNVtxBDf7XebutwE7gGtDj0pEJMk+XLmF94o385PzjlK10UCzFYe71xIZPl1EpM2YMms5vbu05+snDU12KCkn1j6O+WY2A3gW2Fnf6O7PhxKViEgSFZRs4Z2izfz43KPomK1qo6FYE0cHIs/7Hh/V5oASh4iknSmzCsnpnM3XT1bfRmNivXNc/Roi0ibMXbWVtwo3cceXjqRTdswPSW1TYr1z/FEiFcZ+3P2bLR6RiEgSTZlVSK/O2Vx5ivo2mhJrOn0x6n0H4GJgXcuHIyKSPPNWb+XN5WXcrmrjM8V6qurv0dNmNhV4O5SIRESSZMqrQbVxsqqNz3Kwj7AaCfRpyUBERJJpwZpy3lhexrdOH0bn9qo2PkusfRwV7N/H8SmRZ3SIiKSFKa8up0endlx1Sl6yQ0l5MVUc7t7V3btFvQ5vePqqMWY20cyWmVmRmd3eyPwzzGyemdWY2SUN5r1kZuVm9mKD9r+a2UozWxC8RseyDyIiTVm4ppzZy8q4/vTD6KJqo1kxJQ4zu9jMukdN9zCzi5pZJxO4D/gSked4XG5moxosthq4BniqkU38Driyic3/wN1HB68FseyDiEhT7p5VSPeO7bhKV1LFJNY+jp+6+7b6CXcvB37azDrjgCJ3L3b3amAacGH0Au5e4u6LgLqGKwfDuFfEGJ+IyEFZXLqNWZ9s5PrTh9G1Q7tkh9MqxJo4GluuuXpuILAmaro0aGsJvzSzRWb2BzNr39gCZnaDmRWYWUFZWVkLfayIpJsps5bTvWM7rj41L9mhtBqxJo4CM/u9mQ0PXr8H5oYZ2Ge4AzgSGAv0oolOend/0N3z3T0/Nzc3kfGJSCvx0dptvLp0I9edpmojHrEmjpuBauBpIqecqoDvNLPOWiD6Ab2DgrZD4u7rPWI3kUfajjvUbYpI2zRlViHdOmRxzefykh1KqxLrDYA7gQOuimrGHGCkmQ0jkjAmAVfEuY0DmFl/d19vZgZcBHx0qNsUkbZnybptvPLxBr77+ZF0U7URl1ivqnrFzHpETfc0s5c/ax13rwEmAy8DS4Fn3H2Jmd1lZhcE2xlrZqXApcCfzWxJ1Ge8RWQY9wlmVmpmXwxmPWlmi4HFQG/gF7HurIhIvbtnFdK1QxbXfm5YskNpdWK9YLl3cCUVAO6+1cyavXPc3WcCMxu03Rn1fg6RU1iNrXt6E+3jG2sXEYnV0vXbeXnJBm6ZMJLuHVVtxCvWPo46M9s7ML2Z5dHIaLkiIq3B3bMK6do+i+tUbRyUWCuOHwNvm9kbgAGnAzeEFpWISEiWrt/Ovz76lFvGj6B7J1UbByPWzvGXzCyfSLKYD7wA7AozMBGRMNzzWiFd2mfxzdNUbRysWAc5/BZwK5H+iAXAycB77P8oWRGRlLbs0wpmLv6UyWePoEen7GSH02rF2sdxK5Eb7la5+9nAGKD8s1cREUktd79WSOfsTK5TtXFIYk0cVe5eBWBm7d39E+CI8MISEWlZhRsqmLl4PVefmkfPzqo2DkWsneOlwX0cLwCvmNlWYFV4YYmItKy7XyuiY7tMvnX6YckOpdWLtXP84uDtz8xsNtAdeCm0qEREWlDRxgpeXLSOb585nF6qNg5Z3E8scfc3wghERCQsd8+KVBvXq9poEQf7zHERkVahaOMO/rloHVeeMlTVRgtR4hCRtHbva4V0yMrkBlUbLUaJQ0TSVnHZDmYsjFQbOV0afeabHAQlDhFJW/e+VkR2Vob6NlqYEoeIpKWVm3bywoK1fOOkoeR2VbXRkpQ4RCQt3ftaEe0yM7jhTFUbLU2JQ0TSTkl9tXHyUPp07ZDscNKOEoeIpJ17ZxeRlWH8h6qNUChxiEhaWbV5J9Pnr+WKk4ao2giJEoeIpJX7ZheRmWF8+8zhyQ4lbSlxiEjaWLOlkufnreWKcUPo203VRliUOEQkbdw3u4gMU7URNiUOEUkLa7ZU8tzcUiaNG0y/7qo2wqTEISJp4U+vryDDjBvPUrURtlATh5lNNLNlZlZkZrc3Mv8MM5tnZjVmdkmDeS+ZWbmZvdigfZiZfRBs82kz03CXIm1c6dZKni1Yw2VjB9O/e8dkh5P2QkscZpYJ3Ad8CRgFXG5moxosthq4BniqkU38DriykfbfAH9w9xHAVuC6lopZRFqnP72+AjNUbSRImBXHOKDI3YvdvRqYBlwYvYC7l7j7IqCu4cruPguoiG4zMwPGA88FTY8BF4UQuxwEd2fn7hp2VddStaeWPbV11NY57p7s0CSNrS3fxbMFa/ha/mAG9FC1kQhxPwEwDgOBNVHTpcBJh7jNHKDc3WuitjmwsQXN7AbgBoAhQ4Yc4sdKc9Zv28X3nl7A+8VbGp1vBplmZJiRkUHkXzMyDDIyjEwzzIzM6HkZ0esEywbTmRn71s2w+vUJ2vctf8B2M/Z9buPzotsP/Iz9pjOswX4F62cE6x+wfORzoud1ap/F0F6dGNSzI1mZ6nI8GPe/XgTATWePSHIkbUeYiSOp3P1B4EGA/Px8/ckbollLN3DbswvZXVPHLeNH0DE7izqPVBq1dVDnvvdVW0fQ7tQ5Ue3BdF39svvPcydYZt/8/abroNadPbV1e+e5O7XBvAO2FbVe9OfvW2dfDA3XCaOAysowhvTqRF7vzuTldGZY733vB/ToSGaGtfyHpoH123bxzJxSLjlxMANVbSRMmIljLTA4anpQ0HYoNgM9zCwrqDpaYptykKpr6vjtS5/w8NsrOap/N+69YgzDc7skO6zQeWOJqz7R1fmBiSsqITZMmBVVNZRs2knJ5p2s3LSTlZsqeb94M5XVtXs/LzszgyE5nfZLKMNyOpPXuzP9unUgow0nlftfX0GdOzepbyOhwkwcc4CRZjaMyI/7JOCKQ9mgu7uZzQYuIdJncjXwj0MNVOK3enMlN0+dx8LSbVx1ylB+dO5RdGiXmeywEiJyqokWqwLG5vXab9rd2Vixm5WbdlKyaScrN0f+LdlUyVuFZeyu2dcl2KFdBnk5kcokr3eQWHI6M6x3Z3K7tifSLZiePt1WxbQP13Bp/iAG9+qU7HDalNASh7vXmNlk4GUgE3jE3ZeY2V1AgbvPMLOxwHSgJ3C+mf3c3Y8GMLO3gCOBLmZWClzn7i8DPwSmmdkvgPnAX8LaB2nci4vWccffF2MGD3zjBCYe0z/ZIaUVM6Nvtw707daBkw/L2W9eXZ2zfntVJKEEiaVk804KN1Yw65MN7Knddx6tc3YmQ4MkkheVUPJ6dyanc3arTyr3v14UVBvq20g0awtXvOTn53tBQUGyw2j1qvbU8vN/fszUD1czZkgP7p40Rn/ppZDaOmdd+a5IQtkcnVgqWb2lktq6fd/1rh2yIkmkkUqlR6fUvzVqw/YqTv/tbC4ePZDfXHJcssNJW2Y2193zG7anbee4tKzCDRVMfmo+yzZU8O0zh/OfXzicdroKKKVkZhiDe3VicK9OnEHufvP21NZRunXXvkolSCzz12zlxUXriMop9OjUbl91khOpVuorlW4d2iV4rxp3/+srqK1zvqMrqZJCiUM+k7vzbEEpP52xhE7Zmfz12rGcdUSfZIclcWqXmcGw3pFkcHaDebtralmzZVeDTvqdfFC8menz97/2JKdz9t4kEp1Y8nI607l9Yn5ONm6vYuqHq/nKmIEMyVHFmwxKHNKkHbtr+Mn0xbywYB2nHJbDHyeN1lDVaah9ViYj+nRhRJ8Dr4ir2lPLqs2Ve6uU+orlrcIynptbut+yfbq23++Kr+hLilvywokH3iimps6ZPF7VRrIocUijPlq7jZunzmfV5p18/5zD+c7ZI3QvQRvUoV0mR/TryhH9uh4wr7K6hpJNlQf0qcz6ZAObdlTvt+yA7h0iSaRBYhncqxPts2JPKhsrqnjyg1VcNHogQ3M6H/L+ycFR4pD9uDuPvVvCr2Z+Qq/O2Uy9/mROanBljwhAp+wsRg3oxqgB3Q6Yt71qD6s2VUZdShy5rHjm4vWUV+7Zu1yGwYAeHRvtqB/cq9MB/WgPBtXGzao2kkqJQ/Yqr6zm/zy3iH9/vIHxR/bhvy89nl6dU/8KG0k93Tq049hB3Tl2UPcD5pVXVkdVKZV7+1ZeWLCWiqqavctlZhiDe3bce7prcK9OPPHBKi4cPYC83qo2kkmJQwCYu2oLt0xdwMaKKn5y3lFcd9qwVn+dv6SmHp2yGTMkmzFDeu7X7u5s2Vm9N6Gs3LRj76mwD1duobK6lnaZxmRdSZV0ShxtXF2d88CbK/iffy9nQI8OPPftUzl+cI9khyVtkJmR06U9OV3ac+LQA++mL6vYTXVtHYN66kqqZFPiaMPKKnbz/WcW8FbhJs47rj+//sqxKXOdvkg0M6OPruhLGUocbdQ7RZv47tML2L5rD7+6+FguHzdYp6ZEJCZKHG1MTW0df3y1kPteL2J4bhf+dt04jux34FUxIiJNUeJoQ9aV7+LWafOZU7KVr+UP4mcXHE2nbP0vICLx0a9GG/Hqxxu47bmF7Kmp44+XjeaiMY0+OFFEpFlKHGlud00tv/nXMh55ZyVHD+jGvVecwDBdAy8ih0CJI42t2ryTyU/NZ/HabVxzah53nHtkXMM7iIg0RokjTc1YuI4fPb+YDIM/X3kiXzy6X7JDEpE0ocSRZnZV1/Lzfy5h2pw1nDi0J1MmjdYNUyLSopQ40sjyDRVMfmoeyzfs4KazhvO9c/SwJRFpeUocacDdeaZgDT+dsYQu7bN4/JvjOOPw3OZXFBE5CEocrVxF1R5+PP0jZixcx+dG5PCHy0bTp6uGZhCR8ChxtGKLS7cxeeo81myp5LYvHM6NZ+lhSyISPiWOVsjdefSdEn79r6X07tKep//jFMbm9Wp+RRGRFqDE0cqUV1Zz27OLeHXpBj5/VB9+d8nx9NTDlkQkgUK95MbMJprZMjMrMrPbG5l/hpnNM7MaM7ukwbyrzawweF0d1f56sM0FwatPmPuQSuaUbOHcKW/xxvKN/N8vj+Khq/KVNEQk4UKrOMwsE7gPOAcoBeaY2Qx3/zhqsdXANcBtDdbtBfwUyAccmBusuzVY5OvuXhBW7Kmmrs65/40V/P6V5Qzq2ZG/33gqxw3Sw5ZEJDnCPFU1Dihy92IAM5sGXAjsTRzuXhLMq2uw7heBV9x9SzD/FWAiMDXEeFPSxooqvv/0Qt4u2sT5xw/gVxcfQ1c9bElEkijMxDEQWBM1XQqcdAjrRg/n+qiZ1QJ/B37h7t5wA2Z2A3ADwJAhQ+IIO3W8VVjG955ewI7dNfzXV47lsrF62JKIJF9rvK346+5+LHB68LqysYXc/UF3z3f3/Nzc1nUzXE1tHb996ROueuRDenbK5h/fOY1J44YoaYhISggzcawFBkdNDwraDmldd6//twJ4isgpsbSxtnwXlz34Pn96fQVfO3EwMyafxhH9uiY7LBGRvcI8VTUHGGlmw4j86E8Crohx3ZeBX5lZz2D6C8AdZpYF9HD3TWbWDvgy8GoLx500/17yKT94bhE1tXVMmTSaC0frYUsiknpCSxzuXmNmk4kkgUzgEXdfYmZ3AQXuPsPMxgLTgZ7A+Wb2c3c/2t23mNn/I5J8AO4K2joDLwdJI5NI0ngorH1IlN01tfx65if89d0SjhnYjXsvP4E8PWxJRFKUNdKvnHby8/O9oCA1r95duWknN0+dx0drt3Pt5/K4/Ut62JKIpAYzm+vu+Q3bded4Ev1jwVp+9PxisjIzeOiqfM4Z1TfZIYmINEuJIwkqq2v42YwlPFNQSv7Qnky5fAwDe3RMdlgiIjFR4kiwZZ9GHrZUVLaD75w9nO99/nCy9LAlEWlFlDgSxN2ZNmcNP5uxhK4d2vG3b57EaSN7JzssEZG4KXEkQEXVHu54fjEvLlrP6SN78z9fO14PWxKRVkuJI2SLSsuZ/NR81pbv4gdfPIIbzxxOhh62JCKtmBJHSNydv7y9kt+89Am5Xdrz9A0nk6+HLYlIGlDiCMHWndXc9uxCZn2ykXNG9eV3lxxHj056boaIpAcljhb24cot3DptPpt3VPOz80dx9al5GpxQRNKKEkcLqa1z/jS7iD+8upwhvTrx/E2ncszA7skOS0SkxSlxtICN26v47tMLeHfFZi4cPYBfXnwsXdrr0IpIetKv2yF6Y3kZ//lM5GFLv/3qcVyaP0inpkQkrSlxHKQ9tXX8/pXl3P/6Cg7+I7MAAAhQSURBVI7o25Wp15/MyL56boaIpD8ljoNQurWSW6bOZ97qci4fN4Q7vzyKjtka0VZE2gYljji9vORTfvDsQuoc7rl8DOcfPyDZIYmIJJQSR4yq9tTy65lLeey9VRw3qDv3XD6GoTl62JKItD1KHDEoLtvB5Kfm8/H67Vx32jB+OPFIsrM0oq2ItE1KHM2YPr+Un0z/iHZZGfzl6nwmHKWHLYlI26bE0QR350fTP2Lqh6sZl9eLKZePpn93PWxJRESJowlmxrDenbh5/AhunTBSD1sSEQkocXyGG84YnuwQRERSjv6MFhGRuChxiIhIXEJNHGY20cyWmVmRmd3eyPwzzGyemdWY2SUN5l1tZoXB6+qo9hPNbHGwzbtNA0OJiCRUaInDzDKB+4AvAaOAy81sVIPFVgPXAE81WLcX8FPgJGAc8FMz6xnMvh+4HhgZvCaGtAsiItKIMCuOcUCRuxe7ezUwDbgwegF3L3H3RUBdg3W/CLzi7lvcfSvwCjDRzPoD3dz9fXd34HHgohD3QUREGggzcQwE1kRNlwZth7LuwOB9s9s0sxvMrMDMCsrKymIOWkREPlvado67+4Punu/u+bm5uckOR0QkbYSZONYCg6OmBwVth7Lu2uD9wWxTRERaQJg3AM4BRprZMCI/7pOAK2Jc92XgV1Ed4l8A7nD3LWa23cxOBj4ArgLuaW5jc+fO3WRmq+Leg4jewKaDXDdMiis+iis+iis+6RrX0MYaLdLHHA4zOxf4I5AJPOLuvzSzu4ACd59hZmOB6UBPoAr41N2PDtb9JvCjYFO/dPdHg/Z84K9AR+BfwM0e4k6YWYG754e1/YOluOKjuOKjuOLT1uIKdcgRd58JzGzQdmfU+znsf+operlHgEcaaS8AjmnZSEVEJFZp2zkuIiLhUOJo3oPJDqAJiis+iis+iis+bSquUPs4REQk/ajiEBGRuChxiIhIXJQ4AjGM5NvezJ4O5n9gZnkpEtc1ZlZmZguC17cSENMjZrbRzD5qYr4FIxcXmdkiMzsh7JhijOssM9sWdazubGy5EOIabGazzexjM1tiZrc2skzCj1mMcSX8mJlZBzP70MwWBnH9vJFlEv59jDGuhH8foz4708zmm9mLjcxr2ePl7m3+ReQ+kxXAYUA2sBAY1WCZm4AHgveTgKdTJK5rgHsTfLzOAE4APmpi/rlE7rEx4GTggxSJ6yzgxST8/9UfOCF43xVY3sh/x4QfsxjjSvgxC45Bl+B9OyI3+57cYJlkfB9jiSvh38eoz/4+kZHGD/jv1dLHSxVHRLMj+QbTjwXvnwMmJOBZILHElXDu/iaw5TMWuRB43CPeB3oEIxsnO66kcPf17j4veF8BLOXAwTkTfsxijCvhgmOwI5hsF7waXsWT8O9jjHElhZkNAs4DHm5ikRY9XkocEbGM5Lt3GXevAbYBOSkQF8BXg9Mbz5nZ4EbmJ9qhjIwctlOCUw3/MrOjE/3hwSmCMUT+Wo2W1GP2GXFBEo5ZcNplAbCRyCMWmjxeCfw+xhIXJOf7+Efg/3DgIyrqtejxUuJo/f4J5Ln7cUSeW/JYM8u3ZfOAoe5+PJExzl5I5IebWRfg78B33X17Ij/7szQTV1KOmbvXuvtoIiNLjDOzlBgtIoa4Ev59NLMvAxvdfW7Yn1VPiSMilpF89y5jZllAd2BzsuNy983uvjuYfBg4MeSYYnEoIyOHxt23159q8MhwOO3MrHciPtvM2hH5cX7S3Z9vZJGkHLPm4krmMQs+sxyYzYFP+kzG97HZuJL0ffwccIGZlRA5nT3ezJ5osEyLHi8ljoi9I/maWTaRzqMZDZaZAdQ/+/wS4DUPepqSGVeD8+AXEDlPnWwzgKuCK4VOBra5+/pkB2Vm/erP65rZOCL//4f+YxN85l+Ape7++yYWS/gxiyWuZBwzM8s1sx7B+47AOcAnDRZL+PcxlriS8X109zvcfZC75xH5jXjN3b/RYLEWPV6hDnLYWrh7jZlNJjKce/1IvkssaiRfIl+wv5lZEZEO2EkpEtctZnYBUBPEdU3YcZnZVCJX2/Q2s1Iiz4dvF8T8AJGBLc8FioBK4NqwY4oxrkuAG82sBtgFTEpA8ofIX4RXAouD8+MQGfl5SFRsyThmscSVjGPWH3jMzDKJJKpn3P3FZH8fY4wr4d/HpoR5vDTkiIiIxEWnqkREJC5KHCIiEhclDhERiYsSh4iIxEWJQ0RE4qLEIXKQzKw2ahTUBdbI6MWHsO08a2KUX5Fk030cIgdvVzD8hEiboopDpIWZWYmZ/dbMFgfPbxgRtOeZ2WvBAHizzGxI0N7XzKYHAwkuNLNTg01lmtlDFnn2w7+Du5Uxs1ss8gyNRWY2LUm7KW2YEofIwevY4FTVZVHztrn7scC9REYuhcgggY8FA+A9CdwdtN8NvBEMJHgCsCRoHwnc5+5HA+XAV4P224ExwXa+HdbOiTRFd46LHCQz2+HuXRppLwHGu3txMIjgp+6eY2abgP7uvidoX+/uvc2sDBgUNThe/TDnr7j7yGD6h0A7d/+Fmb0E7CAyUu0LUc+IEEkIVRwi4fAm3sdjd9T7Wvb1SZ4H3EekOpkTjHYqkjBKHCLhuCzq3/eC9++yb3C5rwNvBe9nATfC3gcFdW9qo2aWAQx299nAD4kMj31A1SMSJv2lInLwOkaNKgvwkrvXX5Lb08wWEakaLg/abgYeNbMfAGXsGwH3VuBBM7uOSGVxI9DUkOqZwBNBcjHg7uDZECIJoz4OkRYW9HHku/umZMciEgadqhIRkbio4hARkbio4hARkbgocYiISFyUOEREJC5KHCIiEhclDhERicv/Bx79uzRGPlCmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sentence: 'melt butter large saucepan'\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"loss\"], c=\"orange\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# model = load_model(TEXT_GEN_WEIGHTS_NAME+\".h5\")\n",
    "# model.load_weights(TEXT_GEN_WEIGHTS_NAME+\".hdf5\")\n",
    "# model.compile(loss=LOSS, optimizer=OPTIMIZER)\n",
    "with open(INDEX_TO_WORD_FILE, 'rb') as file:\n",
    "    index_to_word = pickle.load(file)\n",
    "\n",
    "with open(STEPS_BEGINNING_FILE, 'rb') as file:\n",
    "    steps_beginning_idx = pickle.load(file)\n",
    "\n",
    "\n",
    "random_seq_index = np.random.randint(0, len(steps_beginning_idx)-1)\n",
    "random_seq = steps_beginning_idx[random_seq_index]\n",
    "\n",
    "word_sequence = [index_to_word[value] for value in random_seq]\n",
    "random_sen = ' '.join(word_sequence)\n",
    "print(f\"Random sentence: '{random_sen}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejDyr3S0A-jl"
   },
   "outputs": [],
   "source": [
    "def generate_sentences(model, input_sentence: list, steps_count: int=10):\n",
    "\n",
    "    with open(INDEX_TO_WORD_FILE, 'rb') as file:\n",
    "        index_to_word = pickle.load(file)\n",
    "\n",
    "\n",
    "    input_sentence_list = input_sentence.copy()\n",
    "    word_to_index = {v: k for k, v in index_to_word.items()}\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words_idx = [word_to_index[w] for w in stop_words if w in word_to_index.keys()]\n",
    "\n",
    "    sentences_delimiter = word_to_index[\".\"]\n",
    "    generated_sen_count = input_sentence_list.count(sentences_delimiter)\n",
    "    input_seq_length = len(input_sentence_list) # model.input_shape[1]\n",
    "\n",
    "\n",
    "    words_count = 0\n",
    "    while generated_sen_count <= steps_count:\n",
    "        input_sentence_arr = np.reshape(input_sentence_list[-input_seq_length:], (1, input_seq_length, 1))\n",
    "\n",
    "        predicted_word_index = model.predict(input_sentence_arr, verbose=0)\n",
    "        predicted_idx_asc = np.argsort(predicted_word_index, axis=1, kind='quicksort')\n",
    "        predicted_word_id = predicted_idx_asc[:,-1][0]\n",
    "        keeped_length_words = min(len(input_sentence_list), 15)\n",
    "        last_N_words = input_sentence_list[-keeped_length_words:]\n",
    "        i = 2\n",
    "        while predicted_word_id not in stop_words_idx and predicted_word_id in last_N_words\\\n",
    "                and predicted_word_id not in [word_to_index[\".\"], word_to_index[\",\"]] :\n",
    "            i += 1\n",
    "\n",
    "        input_sentence_list.append(predicted_word_id)\n",
    "        if predicted_word_id == sentences_delimiter : generated_sen_count += 1\n",
    "        words_count += 1\n",
    "        if words_count >= 50: break\n",
    "\n",
    "    words_seq = [index_to_word[index] for index in input_sentence_list]\n",
    "    final_output = \" \".join(words_seq)\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "generate_sentences(model, random_seq, steps_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIXRAllVA-jl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_gen_training_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cookix",
   "language": "python",
   "name": "cookix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
